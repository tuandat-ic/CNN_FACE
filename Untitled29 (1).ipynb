{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "VT9vahmQWPFT"
      },
      "outputs": [],
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D, Dense, Flatten, Dropout\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1) Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import os\n",
        "src_path = \"/content/drive/MyDrive/data_mau\"\n",
        "\n",
        "# Ki·ªÉm tra th∆∞ m·ª•c\n",
        "print(\"Classes:\", os.listdir(src_path))\n",
        "\n",
        "# 2) Import + t·∫°o generator v·ªõi resize 60x60 v√† t√°ch validation 20%\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    validation_split=0.2,  # 20% ·∫£nh l√†m validation\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest'\n",
        ")\n",
        "\n",
        "train_generator = datagen.flow_from_directory(\n",
        "    src_path,\n",
        "    target_size=(60, 60),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical',\n",
        "    subset='training',   # train subset\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "val_generator = datagen.flow_from_directory(\n",
        "    src_path,\n",
        "    target_size=(60, 60),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical',\n",
        "    subset='validation',  # validation subset\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "# 3) Build CNN model\n",
        "from tensorflow.keras import layers, models\n",
        "\n",
        "num_classes = train_generator.num_classes\n",
        "\n",
        "model = models.Sequential([\n",
        "    layers.Input(shape=(60, 60, 3)),\n",
        "    layers.Conv2D(32, (3,3), activation='relu'),\n",
        "    layers.MaxPooling2D((2,2)),\n",
        "    layers.Conv2D(64, (3,3), activation='relu'),\n",
        "    layers.MaxPooling2D((2,2)),\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(64, activation='relu'),\n",
        "    layers.Dense(num_classes, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.summary()\n",
        "\n",
        "# 4) Train model\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    validation_data=val_generator,\n",
        "    epochs= 200\n",
        ")\n",
        "model.save('final_model.h5')"
      ],
      "metadata": {
        "collapsed": true,
        "id": "kwmffGgBWPpX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================\n",
        "# FIX: ƒê·ªìng b·ªô nh√£n model <-> t√™n hi·ªÉn th·ªã <-> m√¥ t·∫£ (robust)\n",
        "# D√°n nguy√™n v√†o Colab v√† ch·∫°y\n",
        "# =============================\n",
        "\n",
        "from google.colab import files\n",
        "import os, json, unicodedata, glob\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import gradio as gr\n",
        "from PIL import Image\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "# ---------- C·∫•u h√¨nh (ch·ªâ ƒë·ªïi model_path n·∫øu c·∫ßn) ----------\n",
        "model_path = \"final_model.h5\"\n",
        "# N·∫øu b·∫°n c√≥ fallback labels v√¨ ch·∫Øc ch·∫Øn bi·∫øt th·ª© t·ª± model training, gi·ªØ trong labels_fallback.\n",
        "# Nh∆∞ng code s·∫Ω ∆∞u ti√™n t√¨m class_indices t·ª´ train_generator n·∫øu c√≥.\n",
        "labels_fallback = [\"NHUAN\", \"LINH\", \"DAT\"]\n",
        "\n",
        "# N·∫øu b·∫°n ƒë√£ c√≥ face_info (m√¥ t·∫£) v·ªõi key d·∫°ng c√≥ d·∫•u ho·∫∑c kh√¥ng d·∫•u,\n",
        "# paste v√†o ƒë√¢y ch√≠nh x√°c nh∆∞ b·∫°n ƒë√£ vi·∫øt (m√¨nh s·∫Ω auto map).\n",
        "face_info_user = {\n",
        "    \"NHU·∫¨N\": \"Ho√†ng Nhu·∫≠n v·ªõi MSSV 31241024710, l√† m·ªôt ng∆∞·ªùi ƒëam m√™ c√¥ng ngh·ªá v√† th·ªÉ thao. Nam th∆∞·ªùng d√†nh th·ªùi gian ƒë·ªÉ h·ªçc t·∫≠p, t√¨m hi·ªÉu v·ªÅ c√°c m·∫°ch ƒëi·ªán v√† h·ªá th·ªëng th√¥ng minh v√† c≈©ng tham gia c√°c tr·∫≠n b√≥ng chuy·ªÅn s√¥i ƒë·ªông. V·ªÅ ngo·∫°i h√¨nh, m√¨nh cao kho·∫£ng 170 cm v√† n·∫∑ng 80 kg. L√† m·ªôt ng∆∞·ªùi s·ªëng h√≤a ƒë·ªìng, s√¥i n·ªïi, nhi·ªát t√¨nh, lu√¥n s·∫µn s√†ng l·∫Øng nghe v√† h·ªó tr·ª£ b·∫°n b√® trong h·ªçc t·∫≠p v√† c√°c d·ª± √°n s√°ng t·∫°o.\",\n",
        "    \"LINH\": \"Th√πy Linh, hi·ªán l√† sinh vi√™n v·ªõi MSSV 31241023167. Linh c√≥ ni·ªÅm ƒëam m√™ v·ªõi ƒë·ªçc s√°ch v√† ch∆°i c·∫ßu l√¥ng, v√† th∆∞·ªùng d√†nh th·ªùi gian r·∫£nh ƒë·ªÉ kh√°m ph√° th√™m nh·ªØng k·ªπ nƒÉng m·ªõi ho·∫∑c th·ª≠ s·ª©c v·ªõi nh·ªØng tr√≤ ch∆°i s√°ng t·∫°o. V·ªÅ ngo·∫°i h√¨nh, m√¨nh cao kho·∫£ng 160 cm v√† n·∫∑ng 48 kg. M√¨nh l√† ng∆∞·ªùi h√≤a ƒë·ªìng, th√≠ch k·∫øt n·ªëi v·ªõi m·ªçi ng∆∞·ªùi v√† lu√¥n s·∫µn s√†ng tham gia c√°c ho·∫°t ƒë·ªông nh√≥m ho·∫∑c d·ª± √°n m·ªõi ƒë·ªÉ tr·∫£i nghi·ªám v√† ph√°t tri·ªÉn b·∫£n th√¢n.\",\n",
        "    \"ƒê·∫†T\": \"Tu·∫•n ƒê·∫°t ‚Äì MSSV 31241021943 ‚Äì l√† m·ªôt ng∆∞·ªùi tr·∫ª ƒë·∫ßy nhi·ªát huy·∫øt, lu√¥n nu√¥i d∆∞·ª°ng ni·ªÅm ƒëam m√™ v·ªõi c√¥ng ngh·ªá v√† th·ªÉ thao. Ngo√†i gi·ªù h·ªçc, ƒê·∫°t th∆∞·ªùng d√†nh th·ªùi gian ƒë·ªÉ kh√°m ph√° tri th·ª©c m·ªõi, t√¨m hi·ªÉu v·ªÅ cu·ªôc s·ªëng v√† m·ªü r·ªông g√≥c nh√¨n. V·ªõi ngo·∫°i h√¨nh c√¢n ƒë·ªëi (cao kho·∫£ng 1m70, n·∫∑ng 65kg), ƒê·∫°t mang phong th√°i nƒÉng ƒë·ªông, kh·ªèe kho·∫Øn. Trong giao ti·∫øp, ƒê·∫°t ƒë∆∞·ª£c bi·∫øt ƒë·∫øn l√† ng∆∞·ªùi h√≤a ƒë·ªìng, s√¥i n·ªïi v√† nhi·ªát t√¨nh. Kh√¥ng ch·ªâ hƒÉng h√°i tham gia ho·∫°t ƒë·ªông t·∫≠p th·ªÉ, ƒê·∫°t c√≤n lu√¥n s·∫µn s√†ng l·∫Øng nghe, chia s·∫ª v√† h·ªó tr·ª£ b·∫°n b√® trong h·ªçc t·∫≠p c≈©ng nh∆∞ c√°c d·ª± √°n s√°ng t·∫°o.\"\n",
        "}\n",
        "\n",
        "# N·∫øu b·∫°n mu·ªën hi·ªÉn th·ªã t√™n c√≥ d·∫•u (ƒë·∫πp) kh√°c v·ªõi nh√£n g·ªëc, khai b√°o ·ªü ƒë√¢y.\n",
        "# Key ph·∫£i l√† nh√£n g·ªëc (nh∆∞ 'NHUAN'), value l√† t√™n hi·ªÉn th·ªã 'NHU·∫¨N'.\n",
        "label_display_user = {\n",
        "    \"NHUAN\": \"NHU·∫¨N\",\n",
        "    \"LINH\": \"LINH\",\n",
        "    \"DAT\": \"ƒê·∫†T\"\n",
        "}\n",
        "\n",
        "# ----------------- Helpers -----------------\n",
        "def norm_nfc(s):\n",
        "    if s is None:\n",
        "        return None\n",
        "    return unicodedata.normalize(\"NFC\", str(s)).strip()\n",
        "\n",
        "def remove_diacritics(s):\n",
        "    if s is None:\n",
        "        return None\n",
        "    s = unicodedata.normalize(\"NFD\", str(s))\n",
        "    return \"\".join(ch for ch in s if not unicodedata.combining(ch))\n",
        "\n",
        "# Load model\n",
        "print(\"Loading model:\", model_path)\n",
        "model = load_model(model_path)\n",
        "print(\"Model loaded.\")\n",
        "\n",
        "# ----------------- T√¨m labels theo index (an to√†n) -----------------\n",
        "def build_labels_by_index():\n",
        "    # 1) N·∫øu train_generator c√≥ s·∫µn trong scope (n·∫øu b·∫°n ƒëang ch·∫°y cell tr∆∞·ªõc ƒë√£ t·∫°o train_generator)\n",
        "    try:\n",
        "        if 'train_generator' in globals():\n",
        "            ci = train_generator.class_indices\n",
        "            print(\"Found train_generator.class_indices:\", ci)\n",
        "            # build list index -> label\n",
        "            max_idx = max(ci.values()) if len(ci)>0 else -1\n",
        "            labels_by_index = [None]*(max_idx+1)\n",
        "            for k,v in ci.items():\n",
        "                labels_by_index[v] = k\n",
        "            return labels_by_index\n",
        "    except Exception as e:\n",
        "        print(\"train_generator lookup error:\", e)\n",
        "\n",
        "    # 2) Try loading class_indices.json if exists\n",
        "    for fname in [\"class_indices.json\", \"class_indices.txt\", \"class_indices.pkl\"]:\n",
        "        if os.path.exists(fname):\n",
        "            try:\n",
        "                with open(fname, \"r\", encoding=\"utf-8\") as f:\n",
        "                    data = json.load(f)\n",
        "                # expect dict label->index\n",
        "                max_idx = max(data.values())\n",
        "                labels_by_index = [None]*(max_idx+1)\n",
        "                for k,v in data.items():\n",
        "                    labels_by_index[v] = k\n",
        "                print(\"Loaded class indices from\", fname)\n",
        "                return labels_by_index\n",
        "            except Exception:\n",
        "                pass\n",
        "\n",
        "    # 3) Try scanning folders common: 'train', 'data', 'dataset'\n",
        "    for d in [\"./train\", \"./data\", \"./dataset\", \"./training\", \"./images\"]:\n",
        "        if os.path.isdir(d):\n",
        "            sub = [name for name in os.listdir(d) if os.path.isdir(os.path.join(d,name))]\n",
        "            if sub:\n",
        "                sub_sorted = sorted(sub)\n",
        "                print(\"Found training subfolders in\", d, \"->\", sub_sorted)\n",
        "                # CAVEAT: order might not match original training but best-effort\n",
        "                return sub_sorted\n",
        "\n",
        "    # 4) Fallback to labels_fallback if defined\n",
        "    if labels_fallback and isinstance(labels_fallback, list):\n",
        "        print(\"Using labels_fallback (developer-provided):\", labels_fallback)\n",
        "        return labels_fallback\n",
        "\n",
        "    # 5) Last resort: build generic names from number of classes model outputs\n",
        "    try:\n",
        "        # pass a dummy through model to get output size\n",
        "        dummy = np.zeros((1,60,60,3), dtype=np.float32)\n",
        "        preds = model.predict(dummy)\n",
        "        n = preds.shape[1]\n",
        "        generic = [f\"class_{i}\" for i in range(n)]\n",
        "        print(\"No labels found; using generic labels:\", generic)\n",
        "        return generic\n",
        "    except Exception as e:\n",
        "        print(\"Unable to infer labels_by_index:\", e)\n",
        "        return []\n",
        "\n",
        "labels_by_index = build_labels_by_index()\n",
        "labels_by_index = [norm_nfc(lbl) for lbl in labels_by_index]  # normalize\n",
        "print(\"Labels by index (final):\", labels_by_index)\n",
        "\n",
        "# ----------------- Chu·∫©n ho√° face_info v√† map keys -----------------\n",
        "# Normalize user's face_info keys for flexible matching\n",
        "face_info_norm_map = {}   # key_norm -> (orig_key, value)\n",
        "for k,v in face_info_user.items():\n",
        "    kn = norm_nfc(k)\n",
        "    face_info_norm_map[kn] = (k, v)\n",
        "\n",
        "# Build ascii (no-diacritics) map to match NHU·∫¨N <-> NHUAN\n",
        "face_info_ascii_map = {}\n",
        "for kn, (origk, val) in face_info_norm_map.items():\n",
        "    ascii_k = remove_diacritics(kn).upper().strip()\n",
        "    face_info_ascii_map[ascii_k] = (origk, val)\n",
        "\n",
        "# Normalize label_display\n",
        "label_display_norm = {}\n",
        "for k,v in label_display_user.items():\n",
        "    label_display_norm[norm_nfc(k)] = v\n",
        "\n",
        "# Final mapping: label_raw -> description\n",
        "final_face_info = {}\n",
        "mapping_debug = []  # list of tuples for print: (label_raw, matched_key_used or None, how)\n",
        "\n",
        "for lbl in labels_by_index:\n",
        "    lbl_n = norm_nfc(lbl)\n",
        "    lbl_ascii = remove_diacritics(lbl_n).upper().strip()\n",
        "\n",
        "    matched = None\n",
        "    how = None\n",
        "\n",
        "    # 1) Exact match by normalized key (face_info has key identical to label raw)\n",
        "    if lbl_n in face_info_norm_map:\n",
        "        matched = face_info_norm_map[lbl_n][1]\n",
        "        how = \"exact_norm\"\n",
        "    else:\n",
        "        # 2) Match by ascii (remove diacritics)\n",
        "        if lbl_ascii in face_info_ascii_map:\n",
        "            matched = face_info_ascii_map[lbl_ascii][1]\n",
        "            how = \"matched_by_ascii\"\n",
        "        else:\n",
        "            # 3) Try matching by label_display value if user used display name as key in face_info\n",
        "            disp = label_display_norm.get(lbl_n)\n",
        "            if disp:\n",
        "                disp_norm = norm_nfc(disp)\n",
        "                disp_ascii = remove_diacritics(disp_norm).upper().strip()\n",
        "                # check exact/ ascii\n",
        "                if disp_norm in face_info_norm_map:\n",
        "                    matched = face_info_norm_map[disp_norm][1]\n",
        "                    how = \"matched_by_display_exact\"\n",
        "                elif disp_ascii in face_info_ascii_map:\n",
        "                    matched = face_info_ascii_map[disp_ascii][1]\n",
        "                    how = \"matched_by_display_ascii\"\n",
        "    if matched is None:\n",
        "        # create placeholder so UI never shows \"ch∆∞a c√≥ m√¥ t·∫£\"\n",
        "        display_name = label_display_norm.get(lbl_n, lbl_n)\n",
        "        matched = f\"(PLACEHOLDER) Vui l√≤ng c·∫≠p nh·∫≠t m√¥ t·∫£ cho {display_name} (label g·ªëc: {lbl_n}).\"\n",
        "        how = \"placeholder_created\"\n",
        "\n",
        "    final_face_info[lbl_n] = matched\n",
        "    mapping_debug.append((lbl_n, how))\n",
        "\n",
        "# Print mapping debug\n",
        "print(\"\\n=== Mapping debug (label_raw -> how matched) ===\")\n",
        "for lbl_n, how in mapping_debug:\n",
        "    print(f\"  {lbl_n}  -> {how}\")\n",
        "print(\"If you see 'placeholder_created' for any label, please update face_info_user for that label.\\n\")\n",
        "\n",
        "# ----------------- Validate model output size vs labels length -----------------\n",
        "# get n_classes from model output\n",
        "try:\n",
        "    dummy = np.zeros((1,60,60,3), dtype=np.float32)\n",
        "    preds_dummy = model.predict(dummy)\n",
        "    n_classes = preds_dummy.shape[1]\n",
        "except Exception:\n",
        "    # fallback: try to get from last layer\n",
        "    try:\n",
        "        n_classes = model.output_shape[-1]\n",
        "    except Exception:\n",
        "        n_classes = None\n",
        "\n",
        "if n_classes is not None and n_classes != len(labels_by_index):\n",
        "    print(\"‚ö†Ô∏è C·∫¢NH B√ÅO: s·ªë l·ªõp model ({} ) kh√°c s·ªë nh√£n t√¨m ƒë∆∞·ª£c ({}).\".format(n_classes, len(labels_by_index)))\n",
        "    print(\"ƒêi·ªÅu n√†y c√≥ th·ªÉ l√†m k·∫øt qu·∫£ b·ªã l·ªách n·∫øu nh√£n_by_index kh√¥ng ƒë√∫ng th·ª© t·ª± nh∆∞ model ƒë√£ train.\")\n",
        "    # If mismatch, try to resize labels_by_index to n_classes\n",
        "    if n_classes and len(labels_by_index) < n_classes:\n",
        "        # pad with generic names\n",
        "        for i in range(len(labels_by_index), n_classes):\n",
        "            labels_by_index.append(f\"class_{i}\")\n",
        "            final_face_info[f\"class_{i}\"] = f\"(PLACEHOLDER) class_{i}\"\n",
        "        print(\"ƒê√£ b·ªï sung nh√£n generic ƒë·∫øn ƒë·ªß s·ªë l·ªõp model.\")\n",
        "    elif n_classes and len(labels_by_index) > n_classes:\n",
        "        labels_by_index = labels_by_index[:n_classes]\n",
        "        print(\"ƒê√£ c·∫Øt nh√£n_by_index ƒë·ªÉ kh·ªõp s·ªë l·ªõp model.\")\n",
        "\n",
        "# Recompute final normalized labels_by_index\n",
        "labels_by_index = [norm_nfc(x) for x in labels_by_index]\n",
        "\n",
        "# ----------------- H√†m d·ª± ƒëo√°n d√πng chung -----------------\n",
        "def predict_from_pil(img_pil, return_top3=False):\n",
        "    # resize to model expected (you used 60x60)\n",
        "    img_resized = img_pil.convert(\"RGB\").resize((60,60))\n",
        "    x = np.array(img_resized) / 255.0\n",
        "    x = np.expand_dims(x, axis=0)\n",
        "\n",
        "    preds = model.predict(x)  # shape (1, n_classes)\n",
        "    probs = preds[0]\n",
        "    class_idx = int(np.argmax(probs))\n",
        "    confidence = float(probs[class_idx])\n",
        "\n",
        "    # safe index -> label\n",
        "    if class_idx < len(labels_by_index):\n",
        "        label_raw = labels_by_index[class_idx]\n",
        "    else:\n",
        "        label_raw = f\"class_{class_idx}\"\n",
        "\n",
        "    label_raw_n = norm_nfc(label_raw)\n",
        "    label_show = label_display_norm.get(label_raw_n, label_raw_n)  # n·∫øu c√≥ mapping hi·ªÉn th·ªã d√πng n√≥\n",
        "\n",
        "    description = final_face_info.get(label_raw_n, f\"(PLACEHOLDER) Hi·ªán ch∆∞a c√≥ m√¥ t·∫£ cho {label_show}.\")\n",
        "\n",
        "    # top-3\n",
        "    top3 = []\n",
        "    try:\n",
        "        top3_idx = np.argsort(probs)[-3:][::-1]\n",
        "        for i in top3_idx:\n",
        "            lbl = labels_by_index[i] if i < len(labels_by_index) else f\"class_{i}\"\n",
        "            top3.append((lbl, float(probs[i])))\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    if return_top3:\n",
        "        return img_resized, label_show, round(confidence*100,2), description, top3\n",
        "    else:\n",
        "        return img_resized, label_show, round(confidence*100,2), description\n",
        "\n",
        "# ----------------- ƒêO·∫†N 1: upload + d·ª± ƒëo√°n nhanh (in debug) -----------------\n",
        "print(\"\\n--- UPLOAD 1 ·∫£nh ƒë·ªÉ test nhanh ---\")\n",
        "uploaded = files.upload()\n",
        "img_path = list(uploaded.keys())[0]\n",
        "img = Image.open(img_path)\n",
        "\n",
        "img_resized, person, conf, description, top3 = predict_from_pil(img, return_top3=True)\n",
        "print(\"D·ª± ƒëo√°n (hi·ªÉn th·ªã):\", person)\n",
        "print(\"ƒê·ªô tin c·∫≠y:\", conf, \"%\")\n",
        "print(\"M√¥ t·∫£ (ƒë√£ map):\", description)\n",
        "print(\"Top-3 (label_raw,prob):\", top3)\n",
        "\n",
        "# Hi·ªÉn th·ªã ·∫£nh g·ªëc\n",
        "plt.imshow(np.array(img))\n",
        "plt.title(f\"D·ª± ƒëo√°n: {person} ({conf:.2f}%)\")\n",
        "plt.axis(\"off\")\n",
        "plt.show()\n",
        "\n",
        "# ----------------- ƒêO·∫†N 2: Gradio UI -----------------\n",
        "def predict_face_gradio(inp_img):\n",
        "    img_resized, person, conf, description = predict_from_pil(inp_img, return_top3=False)\n",
        "    # show confidence as string with percent\n",
        "    conf_str = f\"{conf:.2f}%\"\n",
        "    return img_resized, person, conf_str, description\n",
        "\n",
        "def reset_image():\n",
        "    return None, None, \"\", \"\", \"\"\n",
        "\n",
        "css_style = \"\"\"\n",
        "body { background: linear-gradient(to right, #ffe0e0, #fff8e1); font-family: 'Poppins', sans-serif;}\n",
        "h1 { color: #d81b60; font-family: 'Orbitron', sans-serif; font-size: 38px;}\n",
        "#predict-btn, #reset-btn { background-color: #f06292; color: #fff; font-weight: bold; font-size: 18px;}\n",
        ".gr-button {border-radius: 20px; padding: 12px 25px;}\n",
        ".gr-box { border: 2px solid #d81b60; border-radius: 20px; padding: 15px; background-color: rgba(255,255,255,0.9); }\n",
        "\"\"\"\n",
        "\n",
        "with gr.Blocks(css=css_style) as demo:\n",
        "    gr.Markdown(\"<h1 style='text-align:center;'>·ª®ng d·ª•ng nh·∫≠n di·ªán khu√¥n m·∫∑t üë§ (Fixed mapping)</h1>\")\n",
        "\n",
        "    with gr.Row():\n",
        "        with gr.Column():\n",
        "            img_input = gr.Image(type=\"pil\", label=\"Ch·ªçn ·∫£nh ho·∫∑c k√©o th·∫£ khu√¥n m·∫∑t v√†o ƒë√¢y\")\n",
        "            btn_predict = gr.Button(\"Nh·∫≠n di·ªán üë§\", elem_id=\"predict-btn\")\n",
        "            btn_reset = gr.Button(\"Ch·ªçn ·∫£nh kh√°c üîÑ\", elem_id=\"reset-btn\")\n",
        "        with gr.Column():\n",
        "            img_resized_output = gr.Image(type=\"pil\", label=\"·∫¢nh ƒë√£ resize (60x60)\")\n",
        "            label_output = gr.Textbox(label=\"T√™n ng∆∞·ªùi d·ª± ƒëo√°n\", interactive=False)\n",
        "            confidence_output = gr.Textbox(label=\"ƒê·ªô ch√≠nh x√°c (%)\", interactive=False)\n",
        "            description_output = gr.Textbox(label=\"M√¥ t·∫£ chi ti·∫øt\", interactive=False, lines=6)\n",
        "\n",
        "    btn_predict.click(\n",
        "        predict_face_gradio,\n",
        "        inputs=img_input,\n",
        "        outputs=[img_resized_output, label_output, confidence_output, description_output]\n",
        "    )\n",
        "\n",
        "    btn_reset.click(\n",
        "        lambda: (None, None, \"\", \"\"),\n",
        "        inputs=None,\n",
        "        outputs=[img_input, img_resized_output, label_output, confidence_output]\n",
        "    )\n",
        "\n",
        "print(\"Launching Gradio app (share=True)...\")\n",
        "demo.launch(share=True)\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "XyTD8U_WYHVb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
